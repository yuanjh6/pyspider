#!/usr/bin/env python
# -*- encoding: utf-8 -*-
# Created on 2019-03-25 11:37:48
# Project: getproxy

from pyspider.libs.base_handler import *

from fake_useragent import UserAgent
 


class Handler(BaseHandler):
    #创建UserAgent对象
    ua = UserAgent(verify_ssl=False)
    crawl_config = {
    }
    

    @every(minutes=24 * 60)
    def on_start(self):
        self.crawl('https://www.xicidaili.com/', callback=self.index_page,headers={
            'Accept':'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',
            'Accept-Encoding':'gzip, deflate',
            'Accept-Language':'zh-CN,zh;q=0.8',
            'Cache-Control':'max-age=0',
            'Connection':'keep-alive',
            'Host':'www.xicidaili.com',
            'User-Agent':ua.random
        })

    @config(age=10 * 24 * 60 * 60)
    def index_page(self, response):
        for each in response.doc('a[href^="http"]').items():
            self.crawl(each.attr.href, callback=self.detail_page)

    @config(priority=2)
    def detail_page(self, response):
        return {
            "url": response.url,
            "title": response.doc('title').text(),
        }

